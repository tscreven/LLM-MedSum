{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3HzxhzzkQp6"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change PATH to desired file location where results will be saved.\n",
    "PATH = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTiBbo5AueXs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoRpZltWVyTo"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "93_06KBPWW-R"
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ccdv/pubmed-summarization\", \"section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7jCEmGd5QL-"
   },
   "outputs": [],
   "source": [
    "model = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "\n",
    "eval_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1763162816848,
     "user": {
      "displayName": "Thomas Screven",
      "userId": "01152057018823631158"
     },
     "user_tz": 480
    },
    "id": "OEXXS15YYLGZ",
    "outputId": "ba902eb7-7e38-4586-9c6a-a25dea6c85d6"
   },
   "outputs": [],
   "source": [
    "train_size = len(ds['train'])\n",
    "val_size = len(ds['validation'])\n",
    "test_size = len(ds['test'])\n",
    "total_size = train_size + val_size + test_size\n",
    "print(f'Number of instances in training set = {train_size}; {train_size / total_size} portion of data')\n",
    "print(f'Number of instances in validation set = {val_size}; {val_size / total_size} portion of data')\n",
    "print(f'Number of instances in test set = {test_size}; {test_size / total_size} portion of data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lBwtHE8slfs"
   },
   "outputs": [],
   "source": [
    "run_abstract_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qyTSfn8iIse"
   },
   "outputs": [],
   "source": [
    "if run_abstract_analysis:\n",
    "  abstract_lengths = []\n",
    "  for abstract in ds['train']['abstract']:\n",
    "    abs_tokens = tokenizer.tokenize(abstract)\n",
    "    abstract_lengths.append(len(abs_tokens))\n",
    "\n",
    "  print(f'Average abstract length = {np.mean(abstract_lengths)}.')\n",
    "  print(f'Max abstract length = {max(abstract_lengths)}.')\n",
    "  print(f'5th percentile abstract length = {np.quantile(abstract_lengths, 0.05)}.')\n",
    "  print(f'25th percentile abstract length = {np.quantile(abstract_lengths, 0.25)}.')\n",
    "  print(f'75th percentile abstract length = {np.quantile(abstract_lengths, 0.75)}.')\n",
    "  print(f'95th percentile abstract length = {np.quantile(abstract_lengths, 0.95)}.')\n",
    "  print(f'99th percentile abstract length = {np.quantile(abstract_lengths, 0.99)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ8Nk3ET5KlQ"
   },
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMsweruEh-Kv"
   },
   "outputs": [],
   "source": [
    "def chunk_paper(text, max_tokens, overlap=50):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens - overlap):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "executionInfo": {
     "elapsed": 335025,
     "status": "error",
     "timestamp": 1763432944914,
     "user": {
      "displayName": "Thomas Screven",
      "userId": "01152057018823631158"
     },
     "user_tz": 480
    },
    "id": "boBWP1RWx1Wh",
    "outputId": "2a51aad7-2d65-4d86-8bcd-ba0b78b7e31c"
   },
   "outputs": [],
   "source": [
    "llm_summaries = []\n",
    "\n",
    "for i in range(200):\n",
    "  paper_chunked = chunk_paper(ds['test'][i]['article'], 512, overlap=64)\n",
    "  chunk_summaries = []\n",
    "  for chunk in paper_chunked:\n",
    "      chunk = \"Summarize:\" + chunk\n",
    "      inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "      summary_ids = model.generate(**inputs, max_length=256, min_length=64)\n",
    "      chunk_summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
    "\n",
    "  all_summaries = ' '.join(chunk_summaries)\n",
    "  final_summary = model.generate( tokenizer(all_summaries, return_tensors=\"pt\", truncation=True, max_length=1024)[\"input_ids\"], min_length=100, max_length=606)\n",
    "  model_summary_text = tokenizer.decode(final_summary[0], skip_special_tokens=True)\n",
    "  print(f'Paper {i}:', model_summary_text)\n",
    "  llm_summaries.append(model_summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgvwF9Qf225X"
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "basename = PATH\n",
    "if save:\n",
    "  np.save(basename+'_text.npy', np.array(llm_summaries))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
