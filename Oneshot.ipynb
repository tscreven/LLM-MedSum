{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3HzxhzzkQp6"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change PATH to file location where you want results to be saved.\n",
    "PATH = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTiBbo5AueXs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoRpZltWVyTo"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "93_06KBPWW-R"
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ccdv/pubmed-summarization\", \"section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7jCEmGd5QL-"
   },
   "outputs": [],
   "source": [
    "model = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "\n",
    "eval_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtA4JaCxmE9N"
   },
   "outputs": [],
   "source": [
    "run_data_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEXXS15YYLGZ"
   },
   "outputs": [],
   "source": [
    "if run_data_analysis:\n",
    "  train_size = len(ds['train'])\n",
    "  val_size = len(ds['validation'])\n",
    "  test_size = len(ds['test'])\n",
    "  total_size = train_size + val_size + test_size\n",
    "  print(f'Number of instances in training set = {train_size}; {train_size / total_size} portion of data')\n",
    "  print(f'Number of instances in validation set = {val_size}; {val_size / total_size} portion of data')\n",
    "  print(f'Number of instances in test set = {test_size}; {test_size / total_size} portion of data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lBwtHE8slfs"
   },
   "outputs": [],
   "source": [
    "run_abstract_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qyTSfn8iIse"
   },
   "outputs": [],
   "source": [
    "if run_abstract_analysis:\n",
    "  abstract_lengths = []\n",
    "  for abstract in ds['train']['abstract']:\n",
    "    abs_tokens = tokenizer.tokenize(abstract)\n",
    "    abstract_lengths.append(len(abs_tokens))\n",
    "\n",
    "  print(f'Average abstract length = {np.mean(abstract_lengths)}.')\n",
    "  print(f'Max abstract length = {max(abstract_lengths)}.')\n",
    "  print(f'5th percentile abstract length = {np.quantile(abstract_lengths, 0.05)}.')\n",
    "  print(f'25th percentile abstract length = {np.quantile(abstract_lengths, 0.25)}.')\n",
    "  print(f'75th percentile abstract length = {np.quantile(abstract_lengths, 0.75)}.')\n",
    "  print(f'95th percentile abstract length = {np.quantile(abstract_lengths, 0.95)}.')\n",
    "  print(f'99th percentile abstract length = {np.quantile(abstract_lengths, 0.99)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ8Nk3ET5KlQ"
   },
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMsweruEh-Kv"
   },
   "outputs": [],
   "source": [
    "def chunk_paper(text, max_tokens=1024, overlap=50):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens - overlap):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrTm9AmQoOQb"
   },
   "outputs": [],
   "source": [
    "# Third cut removed: Example:\\nInput: \"Function fMRI showed activation differences in regions of interest ...\"\\nOutput: \"fMRI shows activation differences in regions.\"\\n\n",
    "query = '''Task: summarize academic paragraphs from medical study.\\n\n",
    "Example:\\nInput: \"This study examines effectiveness of insulin pumps on type one diabetics ...\"\\nOutput: \"Study evaluates insulin pumps.\"\\n\n",
    "Now Summarize:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1763575578656,
     "user": {
      "displayName": "Thomas Screven",
      "userId": "01152057018823631158"
     },
     "user_tz": 480
    },
    "id": "5R61K_TBrbJP",
    "outputId": "3dee2888-ec0e-416c-d632-9a664d40c8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: summarize academic paragraphs from medical study.\n",
      "\n",
      "Example:\n",
      "Input: \"This study examines effectiveness of insulin pumps on type one diabetics ...\"\n",
      "Output: \"Study evaluates insulin pumps.\"\n",
      "\n",
      "Now Summarize:\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yt7D1jTBp52T"
   },
   "outputs": [],
   "source": [
    "chunk_tokens = 512 - len(tokenizer.tokenize(query)) # 512 is max number of tokens allowed in input vector for t5-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21272981,
     "status": "ok",
     "timestamp": 1763596851659,
     "user": {
      "displayName": "Thomas Screven",
      "userId": "01152057018823631158"
     },
     "user_tz": 480
    },
    "id": "boBWP1RWx1Wh",
    "outputId": "9a63ed7d-3964-4218-a5c1-abd640615b79"
   },
   "outputs": [],
   "source": [
    "llm_summaries = []\n",
    "\n",
    "for i in range(200):\n",
    "  paper_chunked = chunk_paper(ds['test'][i]['article'], max_tokens=chunk_tokens, overlap=80)\n",
    "  chunk_summaries = []\n",
    "  for chunk in paper_chunked:\n",
    "      chunk = \"Summarize:\" + chunk\n",
    "      inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "      summary_ids = model.generate(**inputs, max_length=256, min_length=64)\n",
    "      chunk_summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
    "\n",
    "  all_summaries = ' '.join(chunk_summaries)\n",
    "  final_summary = model.generate( tokenizer(all_summaries, return_tensors=\"pt\", truncation=True, max_length=1024)[\"input_ids\"], min_length=100, max_length=606)\n",
    "  model_summary_text = tokenizer.decode(final_summary[0], skip_special_tokens=True)\n",
    "  print(f'Paper {i}:', model_summary_text)\n",
    "  llm_summaries.append(model_summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgvwF9Qf225X"
   },
   "outputs": [],
   "source": [
    "save = True\n",
    "basename = PATH\n",
    "if save:\n",
    "  np.save(basename+'_text.npy', np.array(llm_summaries))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
